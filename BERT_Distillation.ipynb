{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTG7Gd99ZuK/TsjIQPkDb5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidfawazsalihou/Compression-Modele-BERT/blob/main/BERT_Distillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade datasets huggingface_hub fsspec\n",
        "#Puis red√©marrer le runtime"
      ],
      "metadata": {
        "id": "gLcQ0qmxNKjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================= PIPELINE COMPLET ==========================\n",
        "# Knowledge Distillation de BERT vers MiniLM-L6-v2 sur IMDB\n",
        "# 1. Chargement des donn√©es IMDB avec splits stratifi√©s\n",
        "# 2. Chargement des mod√®les teacher et student (MiniLM)\n",
        "# 3. Gel partiel des couches du student (ratio configurable)\n",
        "# 4. Entra√Ænement avec distillation (KL + CE + AMP + early stopping)\n",
        "# 5. Visualisation des m√©triques (loss, accuracy, overfitting)\n",
        "# 6. √âvaluation finale + confusion matrix + rapport\n",
        "# 7. (Optionnel) Fine-tuning post-distillation (√† activer si souhait√©)\n",
        "# 8. Sauvegarde mod√®le final (Torch, Tokenizer, Config)\n",
        "# ======================================================================"
      ],
      "metadata": {
        "id": "ps9fEenNKgGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWWsIh4KGS6A"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation des d√©pendances si n√©cessaire\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except ImportError:\n",
        "    print(\"Installation des biblioth√®ques...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"datasets\", \"transformers\", \"scikit-learn\", \"sentence-transformers\"])\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration du device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ],
      "metadata": {
        "id": "oWD_viTzKYXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# CONFIGURATION OPTIMIS√âE\n",
        "# ======================================================================\n",
        "\n",
        "CONFIG = {\n",
        "    # Mod√®les\n",
        "    \"teacher_model_name\": \"textattack/bert-base-uncased-imdb\",  # BERT fine-tun√©\n",
        "    \"student_model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",  # MiniLM plus petit\n",
        "    \"max_length\": 256,\n",
        "\n",
        "    # Tailles de dataset\n",
        "    \"train_size\": 20000,        # 80% des 25k disponibles\n",
        "    \"validation_size\": 2500,    # 10%\n",
        "    \"test_size\": 2500,          # 10%\n",
        "\n",
        "    # Gel des couches (freeze)\n",
        "    \"freeze_ratio\": 0.5,        # Geler 50% des couches du student\n",
        "\n",
        "    # Hyperparam√®tres d'entra√Ænement\n",
        "    \"batch_size\": 32,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"num_epochs\": 10,\n",
        "    \"warmup_ratio\": 0.1,\n",
        "\n",
        "    # Distillation\n",
        "    \"temperature\": 5.0,\n",
        "    \"alpha\": 0.5,              # Balance KL/CE\n",
        "\n",
        "    # R√©gularisation\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"gradient_clip_norm\": 1.0,\n",
        "    \"label_smoothing\": 0.1,\n",
        "\n",
        "    # Early stopping\n",
        "    \"patience\": 3,\n",
        "    \"min_delta\": 0.001,\n",
        "    \"monitor\": \"val_loss\",\n",
        "\n",
        "    # Optimisation\n",
        "    \"use_amp\": True,           # Mixed precision training\n",
        "    \"num_workers\": 0,          # 0 pour √©viter les erreurs Colab\n",
        "\n",
        "    # Fine-tuning post-distillation (optionnel)\n",
        "    \"enable_post_finetuning\": False,\n",
        "    \"finetuning_epochs\": 3,\n",
        "    \"finetuning_lr\": 1e-5,\n",
        "\n",
        "    # Sauvegarde\n",
        "    \"save_dir\": \"/content/distillation_minilm\",\n",
        "    \"checkpoint_interval\": 2,\n",
        "\n",
        "    # Autres\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "# Cr√©er les dossiers\n",
        "os.makedirs(CONFIG[\"save_dir\"], exist_ok=True)\n",
        "\n",
        "# Fixer la seed\n",
        "torch.manual_seed(CONFIG[\"seed\"])\n",
        "np.random.seed(CONFIG[\"seed\"])\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(CONFIG[\"seed\"])"
      ],
      "metadata": {
        "id": "lrkK0ZbCKp__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# √âTAPE 1 : CHARGEMENT ET PR√âPARATION DES DONN√âES\n",
        "# ======================================================================\n",
        "\n",
        "def load_and_prepare_data(config):\n",
        "    \"\"\"Charge et pr√©pare le dataset IMDB avec splits stratifi√©s\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"√âTAPE 1 : CHARGEMENT DES DONN√âES IMDB\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Charger le dataset\n",
        "    print(\"T√©l√©chargement du dataset IMDB...\")\n",
        "    dataset = load_dataset(\"imdb\")\n",
        "\n",
        "\n",
        "    print(f\"‚úÖ Dataset charg√©!\")\n",
        "    print(f\"   Train original: {len(dataset['train'])} exemples\")\n",
        "    print(f\"   Test original: {len(dataset['test'])} exemples\")\n",
        "\n",
        "    # Extraire les donn√©es\n",
        "    all_texts = list(dataset['train']['text']) + list(dataset['test']['text'])\n",
        "    all_labels = list(dataset['train']['label']) + list(dataset['test']['label'])\n",
        "\n",
        "    # Limiter au total n√©cessaire\n",
        "    total_needed = config['train_size'] + config['validation_size'] + config['test_size']\n",
        "    all_texts = all_texts[:total_needed]\n",
        "    all_labels = all_labels[:total_needed]\n",
        "\n",
        "    # Split stratifi√©\n",
        "    print(f\"\\nCr√©ation des splits stratifi√©s...\")\n",
        "\n",
        "    # D'abord s√©parer train+val vs test\n",
        "    train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(\n",
        "        all_texts, all_labels,\n",
        "        test_size=config['test_size'],\n",
        "        random_state=config['seed'],\n",
        "        stratify=all_labels\n",
        "    )\n",
        "\n",
        "    # Puis s√©parer train vs val\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        train_val_texts, train_val_labels,\n",
        "        test_size=config['validation_size'],\n",
        "        random_state=config['seed'],\n",
        "        stratify=train_val_labels\n",
        "    )\n",
        "\n",
        "    # V√©rifications\n",
        "    print(f\"\\nüìä Distribution des classes:\")\n",
        "    print(f\"   Train: {len(train_texts)} exemples, {sum(train_labels)/len(train_labels)*100:.1f}% positifs\")\n",
        "    print(f\"   Val: {len(val_texts)} exemples, {sum(val_labels)/len(val_labels)*100:.1f}% positifs\")\n",
        "    print(f\"   Test: {len(test_texts)} exemples, {sum(test_labels)/len(test_labels)*100:.1f}% positifs\")\n",
        "\n",
        "    # V√©rifier l'absence de chevauchement\n",
        "    train_set = set(train_texts[:100])\n",
        "    val_set = set(val_texts[:100])\n",
        "    test_set = set(test_texts[:100])\n",
        "\n",
        "    assert len(train_set & val_set) == 0, \"Chevauchement train/val!\"\n",
        "    assert len(train_set & test_set) == 0, \"Chevauchement train/test!\"\n",
        "    assert len(val_set & test_set) == 0, \"Chevauchement val/test!\"\n",
        "    print(\"‚úÖ Pas de chevauchement entre les sets!\")\n",
        "\n",
        "    return {\n",
        "        'train': (train_texts, train_labels),\n",
        "        'validation': (val_texts, val_labels),\n",
        "        'test': (test_texts, test_labels)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "lb7pP8wGKvLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# √âTAPE 2 : CHARGEMENT DES MOD√àLES + GEL DES COUCHES\n",
        "# ======================================================================\n",
        "\n",
        "def freeze_bottom_layers(model, freeze_ratio=0.5):\n",
        "    \"\"\"G√®le un pourcentage des couches du mod√®le pour r√©duire l'overfitting\"\"\"\n",
        "\n",
        "    # Identifier les couches du mod√®le\n",
        "    if hasattr(model, 'bert'):\n",
        "        layers = model.bert.encoder.layer\n",
        "    elif hasattr(model, 'distilbert'):\n",
        "        layers = model.distilbert.transformer.layer\n",
        "    elif hasattr(model, 'roberta'):\n",
        "        layers = model.roberta.encoder.layer\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Architecture non reconnue, pas de gel appliqu√©\")\n",
        "        return model\n",
        "\n",
        "    num_layers = len(layers)\n",
        "    num_to_freeze = int(num_layers * freeze_ratio)\n",
        "\n",
        "    print(f\"\\nüßä Gel des {num_to_freeze} premi√®res couches sur {num_layers}\")\n",
        "\n",
        "    # Geler les couches\n",
        "    for i, layer in enumerate(layers):\n",
        "        if i < num_to_freeze:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # Compter les param√®tres\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"   Param√®tres entra√Ænables: {trainable:,} / {total:,} ({trainable/total:.1%})\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_models(config):\n",
        "    \"\"\"Charge les mod√®les teacher et student avec MiniLM\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"√âTAPE 2 : CHARGEMENT DES MOD√àLES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Charger le teacher (BERT)\n",
        "    print(f\"Chargement du teacher: {config['teacher_model_name']}\")\n",
        "    try:\n",
        "        teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            config[\"teacher_model_name\"]\n",
        "        ).to(device)\n",
        "        teacher_tokenizer = AutoTokenizer.from_pretrained(config[\"teacher_model_name\"])\n",
        "        print(\"‚úÖ Teacher charg√© (BERT fine-tun√© sur IMDB)\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è  Utilisation de BERT standard\")\n",
        "        teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            \"bert-base-uncased\",\n",
        "            num_labels=2\n",
        "        ).to(device)\n",
        "        teacher_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # Charger le student (MiniLM)\n",
        "    print(f\"\\nChargement du student: {config['student_model_name']}\")\n",
        "\n",
        "    # MiniLM n√©cessite une adaptation pour la classification\n",
        "    from transformers import AutoModel\n",
        "\n",
        "    # Cr√©er un mod√®le de classification bas√© sur MiniLM\n",
        "    class MiniLMForSequenceClassification(nn.Module):\n",
        "        def __init__(self, model_name, num_labels=2):\n",
        "            super().__init__()\n",
        "            self.minilm = AutoModel.from_pretrained(model_name)\n",
        "            self.config = self.minilm.config\n",
        "            self.dropout = nn.Dropout(0.1)\n",
        "            self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
        "\n",
        "        def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "            outputs = self.minilm(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            pooled_output = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
        "            pooled_output = self.dropout(pooled_output)\n",
        "            logits = self.classifier(pooled_output)\n",
        "\n",
        "            loss = None\n",
        "            if labels is not None:\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "\n",
        "            return type('Output', (), {'loss': loss, 'logits': logits})()\n",
        "\n",
        "    student_model = MiniLMForSequenceClassification(config[\"student_model_name\"]).to(device)\n",
        "    student_tokenizer = AutoTokenizer.from_pretrained(config[\"student_model_name\"])\n",
        "\n",
        "    # Appliquer le gel des couches\n",
        "    student_model = freeze_bottom_layers(student_model, config[\"freeze_ratio\"])\n",
        "\n",
        "    # Informations sur les mod√®les\n",
        "    teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
        "    student_params = sum(p.numel() for p in student_model.parameters())\n",
        "    student_trainable = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"\\nüìä R√©sum√© des mod√®les:\")\n",
        "    print(f\"   Teacher (BERT): {teacher_params:,} param√®tres\")\n",
        "    print(f\"   Student (MiniLM): {student_params:,} param√®tres totaux\")\n",
        "    print(f\"   Student trainable: {student_trainable:,} param√®tres\")\n",
        "    print(f\"   Ratio de compression: {teacher_params/student_params:.2f}x\")\n",
        "    print(f\"   Ratio params/data: {student_trainable/config['train_size']:.1f}\")\n",
        "\n",
        "    return teacher_model, student_model, teacher_tokenizer, student_tokenizer"
      ],
      "metadata": {
        "id": "m55q2vW7K1EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# √âTAPE 3 : DATASET PYTORCH PERSONNALIS√â\n",
        "# ======================================================================\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    \"\"\"Dataset PyTorch pour IMDB\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "qHLNhInYLkJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ======================================================================\n",
        "# √âTAPE 4 : DUAL DATALOADER POUR SYNCHRONISATION\n",
        "# ======================================================================\n",
        "\n",
        "class DualDataLoader:\n",
        "    \"\"\"DataLoader synchronis√© pour teacher et student\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_teacher, dataset_student, batch_size, shuffle=True, num_workers=0):\n",
        "        self.loader_teacher = DataLoader(\n",
        "            dataset_teacher,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "        self.loader_student = DataLoader(\n",
        "            dataset_student,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.loader_student)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch_t, batch_s in zip(self.loader_teacher, self.loader_student):\n",
        "            yield batch_t, batch_s\n",
        "\n"
      ],
      "metadata": {
        "id": "QCcMg_CGK4-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# √âTAPE 5 : LOSS DE DISTILLATION\n",
        "# ======================================================================\n",
        "\n",
        "class DistillationLoss(nn.Module):\n",
        "    \"\"\"Loss combinant KL divergence et Cross Entropy avec label smoothing\"\"\"\n",
        "\n",
        "    def __init__(self, temperature=5.0, alpha=0.5, label_smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.label_smoothing = label_smoothing\n",
        "        self.kl_div = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "        self.ce_loss = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        # KL divergence loss\n",
        "        student_soft = F.log_softmax(student_logits / self.temperature, dim=-1)\n",
        "        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=-1)\n",
        "        kl_loss = self.kl_div(student_soft, teacher_soft) * (self.temperature ** 2)\n",
        "\n",
        "        # Cross entropy loss\n",
        "        ce_loss = self.ce_loss(student_logits, labels)\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = self.alpha * kl_loss + (1 - self.alpha) * ce_loss\n",
        "\n",
        "        return total_loss, kl_loss, ce_loss\n",
        "\n",
        "# ======================================================================\n",
        "# EARLY STOPPING\n",
        "# ======================================================================\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0.001, monitor='val_loss'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.monitor = monitor\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.best_model_state = None\n",
        "\n",
        "    def __call__(self, score, model):\n",
        "        if 'loss' in self.monitor:\n",
        "            score = -score\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "        elif score < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        self.best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}"
      ],
      "metadata": {
        "id": "p9nZ9jLMK8W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# √âTAPE 6 : ENTRA√éNEMENT AVEC DISTILLATION + AMP\n",
        "# ======================================================================\n",
        "\n",
        "def train_with_dual_loader(teacher_model, student_model, train_loader, val_loader,\n",
        "                          test_loader, config):\n",
        "    \"\"\"Entra√Ænement avec distillation, AMP et monitoring complet\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"√âTAPE 6 : ENTRA√éNEMENT AVEC DISTILLATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Optimizer et scheduler\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [p for p in student_model.parameters() if p.requires_grad],\n",
        "        lr=config[\"learning_rate\"],\n",
        "        weight_decay=config[\"weight_decay\"]\n",
        "    )\n",
        "\n",
        "    num_training_steps = len(train_loader) * config[\"num_epochs\"]\n",
        "    num_warmup_steps = int(num_training_steps * config[\"warmup_ratio\"])\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Loss et scaler pour AMP\n",
        "    distillation_loss_fn = DistillationLoss(\n",
        "        temperature=config[\"temperature\"],\n",
        "        alpha=config[\"alpha\"],\n",
        "        label_smoothing=config[\"label_smoothing\"]\n",
        "    )\n",
        "\n",
        "    scaler = GradScaler() if config[\"use_amp\"] else None\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(\n",
        "        patience=config[\"patience\"],\n",
        "        min_delta=config[\"min_delta\"],\n",
        "        monitor=config[\"monitor\"]\n",
        "    )\n",
        "\n",
        "    # Historique\n",
        "    history = {\n",
        "        \"epoch\": [], \"train_loss\": [], \"val_loss\": [],\n",
        "        \"train_acc\": [], \"val_acc\": [], \"test_acc\": [],\n",
        "        \"kl_loss\": [], \"ce_loss\": [], \"lr\": []\n",
        "    }\n",
        "\n",
        "    # √âvaluation initiale\n",
        "    print(\"\\nüìä √âvaluation initiale...\")\n",
        "    teacher_acc = evaluate_model(teacher_model, test_loader)\n",
        "    student_acc = evaluate_model(student_model, test_loader)\n",
        "    print(f\"Teacher accuracy: {teacher_acc:.4f}\")\n",
        "    print(f\"Student accuracy (avant): {student_acc:.4f}\")\n",
        "\n",
        "    if student_acc > 0.7:\n",
        "        print(\"‚ö†Ô∏è  Accuracy initiale √©lev√©e!\")\n",
        "\n",
        "    best_val_acc = 0\n",
        "\n",
        "    # Boucle d'entra√Ænement\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
        "\n",
        "        # Mode entra√Ænement\n",
        "        student_model.train()\n",
        "        teacher_model.eval()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_kl_loss = 0\n",
        "        epoch_ce_loss = 0\n",
        "        epoch_correct = 0\n",
        "        epoch_total = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "        for batch_idx, (batch_teacher, batch_student) in enumerate(pbar):\n",
        "            # Donn√©es vers GPU\n",
        "            teacher_inputs = {k: v.to(device) for k, v in batch_teacher.items() if k != 'labels'}\n",
        "            student_inputs = {k: v.to(device) for k, v in batch_student.items() if k != 'labels'}\n",
        "            labels = batch_student['labels'].to(device)\n",
        "\n",
        "            # Mixed precision training\n",
        "            if config[\"use_amp\"]:\n",
        "                with autocast():\n",
        "                    # Teacher forward (no grad)\n",
        "                    with torch.no_grad():\n",
        "                        teacher_outputs = teacher_model(**teacher_inputs)\n",
        "                        teacher_logits = teacher_outputs.logits\n",
        "\n",
        "                    # Student forward\n",
        "                    student_outputs = student_model(**student_inputs)\n",
        "                    student_logits = student_outputs.logits\n",
        "\n",
        "                    # Loss\n",
        "                    loss, kl_loss, ce_loss = distillation_loss_fn(\n",
        "                        student_logits, teacher_logits, labels\n",
        "                    )\n",
        "\n",
        "                # Backward avec scaler\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                if (batch_idx + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(\n",
        "                        student_model.parameters(),\n",
        "                        config[\"gradient_clip_norm\"]\n",
        "                    )\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    optimizer.zero_grad()\n",
        "                    scheduler.step()\n",
        "            else:\n",
        "                # Sans AMP\n",
        "                with torch.no_grad():\n",
        "                    teacher_outputs = teacher_model(**teacher_inputs)\n",
        "                    teacher_logits = teacher_outputs.logits\n",
        "\n",
        "                student_outputs = student_model(**student_inputs)\n",
        "                student_logits = student_outputs.logits\n",
        "\n",
        "                loss, kl_loss, ce_loss = distillation_loss_fn(\n",
        "                    student_logits, teacher_logits, labels\n",
        "                )\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                if (batch_idx + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(\n",
        "                        student_model.parameters(),\n",
        "                        config[\"gradient_clip_norm\"]\n",
        "                    )\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            # Statistiques\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_kl_loss += kl_loss.item()\n",
        "            epoch_ce_loss += ce_loss.item()\n",
        "\n",
        "            preds = torch.argmax(student_logits, dim=-1)\n",
        "            epoch_correct += (preds == labels).sum().item()\n",
        "            epoch_total += labels.size(0)\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{epoch_correct/epoch_total:.4f}',\n",
        "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
        "            })\n",
        "\n",
        "        # Moyennes epoch\n",
        "        avg_train_loss = epoch_loss / len(train_loader)\n",
        "        avg_kl_loss = epoch_kl_loss / len(train_loader)\n",
        "        avg_ce_loss = epoch_ce_loss / len(train_loader)\n",
        "        train_acc = epoch_correct / epoch_total\n",
        "\n",
        "        # Validation\n",
        "        print(\"\\nüìä Validation...\")\n",
        "        val_loss, val_acc = evaluate_with_loss(student_model, val_loader, nn.CrossEntropyLoss())\n",
        "        test_acc = evaluate_model(student_model, test_loader)\n",
        "\n",
        "        # Affichage\n",
        "        print(f\"\\nüìà Epoch {epoch+1} Results:\")\n",
        "        print(f\"   Train Loss: {avg_train_loss:.4f} (KL: {avg_kl_loss:.4f}, CE: {avg_ce_loss:.4f})\")\n",
        "        print(f\"   Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"   Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "        print(f\"   Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "        # D√©tection overfitting\n",
        "        if train_acc - val_acc > 0.1:\n",
        "            print(\"   ‚ö†Ô∏è  Overfitting d√©tect√© (√©cart > 10%)\")\n",
        "\n",
        "        # Early stopping\n",
        "        early_stopping(val_loss, student_model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"\\n‚èπÔ∏è  Early stopping!\")\n",
        "            student_model.load_state_dict(early_stopping.best_model_state)\n",
        "            break\n",
        "\n",
        "        # Sauvegarde best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'model_state_dict': student_model.state_dict(),\n",
        "                'epoch': epoch + 1,\n",
        "                'val_acc': val_acc,\n",
        "                'config': config\n",
        "            }, os.path.join(config[\"save_dir\"], \"best_model.pt\"))\n",
        "            print(f\"   ‚úÖ Nouveau meilleur mod√®le! (Val Acc: {best_val_acc:.4f})\")\n",
        "\n",
        "        # Historique\n",
        "        history[\"epoch\"].append(epoch + 1)\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        history[\"test_acc\"].append(test_acc)\n",
        "        history[\"kl_loss\"].append(avg_kl_loss)\n",
        "        history[\"ce_loss\"].append(avg_ce_loss)\n",
        "        history[\"lr\"].append(scheduler.get_last_lr()[0])\n",
        "\n",
        "    return history, teacher_acc\n",
        "\n",
        "# ======================================================================\n",
        "# FONCTIONS D'√âVALUATION\n",
        "# ======================================================================\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    \"\"\"√âvalue l'accuracy du mod√®le\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluation\", leave=False):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            preds = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "def evaluate_with_loss(model, dataloader, loss_fn):\n",
        "    \"\"\"√âvalue le mod√®le avec loss et accuracy\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=-1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "ozsiMGCmLsuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# √âTAPE 7 : VISUALISATION DES R√âSULTATS\n",
        "# ======================================================================\n",
        "\n",
        "def plot_training_results(history):\n",
        "    \"\"\"Visualisation compl√®te des m√©triques d'entra√Ænement\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"√âTAPE 7 : VISUALISATION DES R√âSULTATS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "    # 1. Loss curves\n",
        "    axes[0, 0].plot(history[\"epoch\"], history[\"train_loss\"], 'b-', label='Train', linewidth=2)\n",
        "    axes[0, 0].plot(history[\"epoch\"], history[\"val_loss\"], 'r-', label='Val', linewidth=2)\n",
        "    axes[0, 0].set_title('Total Loss Evolution', fontsize=14)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Loss components\n",
        "    axes[0, 1].plot(history[\"epoch\"], history[\"kl_loss\"], 'c-', label='KL Loss', linewidth=2)\n",
        "    axes[0, 1].plot(history[\"epoch\"], history[\"ce_loss\"], 'm-', label='CE Loss', linewidth=2)\n",
        "    axes[0, 1].set_title('Loss Components', fontsize=14)\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Accuracy\n",
        "    axes[0, 2].plot(history[\"epoch\"], history[\"train_acc\"], 'b-', label='Train', linewidth=2)\n",
        "    axes[0, 2].plot(history[\"epoch\"], history[\"val_acc\"], 'r-', label='Val', linewidth=2)\n",
        "    axes[0, 2].plot(history[\"epoch\"], history[\"test_acc\"], 'g--', label='Test', linewidth=2)\n",
        "    axes[0, 2].set_title('Accuracy Evolution', fontsize=14)\n",
        "    axes[0, 2].set_xlabel('Epoch')\n",
        "    axes[0, 2].set_ylabel('Accuracy')\n",
        "    axes[0, 2].set_ylim(0.5, 1.0)\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Overfitting detection\n",
        "    overfitting_gap = [t - v for t, v in zip(history[\"train_acc\"], history[\"val_acc\"])]\n",
        "    axes[1, 0].plot(history[\"epoch\"], overfitting_gap, 'orange', linewidth=2)\n",
        "    axes[1, 0].axhline(y=0.1, color='r', linestyle='--', label='Threshold (10%)')\n",
        "    axes[1, 0].set_title('Overfitting Detection (Train-Val Gap)', fontsize=14)\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Accuracy Gap')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 5. Learning rate\n",
        "    axes[1, 1].plot(history[\"epoch\"], history[\"lr\"], 'purple', linewidth=2)\n",
        "    axes[1, 1].set_title('Learning Rate Schedule', fontsize=14)\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Learning Rate')\n",
        "    axes[1, 1].set_yscale('log')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. Loss ratio (KL/CE)\n",
        "    loss_ratio = [kl/ce if ce > 0 else 0 for kl, ce in zip(history[\"kl_loss\"], history[\"ce_loss\"])]\n",
        "    axes[1, 2].plot(history[\"epoch\"], loss_ratio, 'brown', linewidth=2)\n",
        "    axes[1, 2].set_title('KL/CE Loss Ratio', fontsize=14)\n",
        "    axes[1, 2].set_xlabel('Epoch')\n",
        "    axes[1, 2].set_ylabel('Ratio')\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(CONFIG[\"save_dir\"], \"training_curves.png\"), dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"‚úÖ Graphiques sauvegard√©s!\")"
      ],
      "metadata": {
        "id": "CmxI41MQLxLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# √âTAPE 8 : √âVALUATION FINALE + ANALYSE\n",
        "# ======================================================================\n",
        "\n",
        "def analyze_final_results(student_model, test_loader, history, teacher_accuracy):\n",
        "    \"\"\"Analyse d√©taill√©e des r√©sultats finaux avec matrice de confusion\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"√âTAPE 8 : √âVALUATION FINALE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # √âvaluation compl√®te sur test\n",
        "    student_model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Final evaluation\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = student_model(**inputs)\n",
        "            probs = F.softmax(outputs.logits, dim=-1)\n",
        "            preds = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # Accuracy finale\n",
        "    final_accuracy = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)\n",
        "\n",
        "    # Matrice de confusion\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "                xticklabels=['Negative', 'Positive'],\n",
        "                yticklabels=['Negative', 'Positive'])\n",
        "    plt.title('Confusion Matrix - Student Model')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig(os.path.join(CONFIG[\"save_dir\"], \"confusion_matrix.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Rapport de classification\n",
        "    print(\"\\nüìä Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds,\n",
        "                              target_names=['Negative', 'Positive'],\n",
        "                              digits=4))\n",
        "\n",
        "    # Analyse de la distillation\n",
        "    print(\"\\nüìà R√âSUM√â DE LA DISTILLATION:\")\n",
        "    print(f\"   Teacher accuracy: {teacher_accuracy:.4f}\")\n",
        "    print(f\"   Student accuracy finale: {final_accuracy:.4f}\")\n",
        "    print(f\"   Performance retention: {final_accuracy/teacher_accuracy:.1%}\")\n",
        "\n",
        "    # Analyse de la distribution des probabilit√©s\n",
        "    probs_array = np.array(all_probs)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Histogramme des probabilit√©s pour la classe positive\n",
        "    plt.subplot(1, 2, 1)\n",
        "    pos_indices = [i for i, l in enumerate(all_labels) if l == 1]\n",
        "    neg_indices = [i for i, l in enumerate(all_labels) if l == 0]\n",
        "\n",
        "    plt.hist([probs_array[pos_indices, 1], probs_array[neg_indices, 1]],\n",
        "             bins=30, alpha=0.6, label=['True Positive', 'True Negative'])\n",
        "    plt.xlabel('Probability of Positive Class')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Probability Distribution by True Class')\n",
        "    plt.legend()\n",
        "    plt.axvline(x=0.5, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Courbe de calibration\n",
        "    plt.subplot(1, 2, 2)\n",
        "    from sklearn.calibration import calibration_curve\n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
        "        all_labels, probs_array[:, 1], n_bins=10\n",
        "    )\n",
        "    plt.plot(mean_predicted_value, fraction_of_positives, 'o-', label='Student')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
        "    plt.xlabel('Mean Predicted Probability')\n",
        "    plt.ylabel('Fraction of Positives')\n",
        "    plt.title('Calibration Plot')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(CONFIG[\"save_dir\"], \"probability_analysis.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # V√©rifications finales\n",
        "    print(\"\\nüîç V√©rifications finales:\")\n",
        "\n",
        "    # Overfitting\n",
        "    final_train_acc = history['train_acc'][-1] if history['train_acc'] else 0\n",
        "    final_val_acc = history['val_acc'][-1] if history['val_acc'] else 0\n",
        "    overfitting_gap = final_train_acc - final_val_acc\n",
        "\n",
        "    if overfitting_gap > 0.1:\n",
        "        print(f\"   ‚ö†Ô∏è  Overfitting: √©cart train-val de {overfitting_gap:.1%}\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ Pas d'overfitting significatif (√©cart: {overfitting_gap:.1%})\")\n",
        "\n",
        "    # Performance\n",
        "    if final_accuracy > 0.98:\n",
        "        print(\"   ‚ö†Ô∏è  Accuracy suspicieusement √©lev√©e (>98%)\")\n",
        "    elif final_accuracy < 0.7:\n",
        "        print(\"   ‚ö†Ô∏è  Performance faible (<70%)\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ Performance normale ({final_accuracy:.1%})\")\n",
        "\n",
        "    return final_accuracy\n"
      ],
      "metadata": {
        "id": "_wluiH7rL0tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# √âTAPE 9 : SAUVEGARDE FINALE\n",
        "# ======================================================================\n",
        "\n",
        "def save_final_model(student_model, student_tokenizer, history, config):\n",
        "    \"\"\"Sauvegarde compl√®te du mod√®le et des r√©sultats\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"√âTAPE 9 : SAUVEGARDE FINALE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Sauvegarder le mod√®le\n",
        "    model_path = os.path.join(config[\"save_dir\"], \"student_model_final\")\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "    # Sauvegarder les poids\n",
        "    torch.save(student_model.state_dict(),\n",
        "               os.path.join(model_path, \"pytorch_model.bin\"))\n",
        "\n",
        "    # Sauvegarder le tokenizer\n",
        "    student_tokenizer.save_pretrained(model_path)\n",
        "\n",
        "    # Sauvegarder la configuration\n",
        "    model_config = {\n",
        "        \"model_type\": \"MiniLM-distilled\",\n",
        "        \"teacher_model\": config[\"teacher_model_name\"],\n",
        "        \"student_model\": config[\"student_model_name\"],\n",
        "        \"num_labels\": 2,\n",
        "        \"freeze_ratio\": config[\"freeze_ratio\"],\n",
        "        \"final_accuracy\": history['test_acc'][-1] if history['test_acc'] else 0\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(model_path, \"model_config.json\"), \"w\") as f:\n",
        "        json.dump(model_config, f, indent=2)\n",
        "\n",
        "    # Sauvegarder l'historique\n",
        "    with open(os.path.join(config[\"save_dir\"], \"training_history.json\"), \"w\") as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "\n",
        "    # Sauvegarder la configuration d'entra√Ænement\n",
        "    with open(os.path.join(config[\"save_dir\"], \"training_config.json\"), \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Mod√®le sauvegard√© dans: {model_path}\")\n",
        "    print(f\"‚úÖ Historique sauvegard√©: training_history.json\")\n",
        "    print(f\"‚úÖ Configuration sauvegard√©e: training_config.json\")\n",
        "\n",
        "    # Instructions pour charger le mod√®le\n",
        "    print(\"\\nüìù Pour charger le mod√®le:\")\n",
        "    print(f\"\"\"\n",
        "    # Charger le tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"{model_path}\")\n",
        "\n",
        "    # Recr√©er le mod√®le\n",
        "    model = MiniLMForSequenceClassification(\"{config['student_model_name']}\")\n",
        "    model.load_state_dict(torch.load(\"{os.path.join(model_path, 'pytorch_model.bin')}\"))\n",
        "    model.eval()\n",
        "    \"\"\")\n"
      ],
      "metadata": {
        "id": "1U_xoZkYL35C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# √âTAPE 10 (OPTIONNELLE) : FINE-TUNING POST-DISTILLATION\n",
        "# ======================================================================\n",
        "\n",
        "def post_distillation_finetuning(student_model, train_loader, val_loader, config):\n",
        "    \"\"\"Fine-tuning optionnel apr√®s distillation sur les hard labels\"\"\"\n",
        "\n",
        "    if not config.get(\"enable_post_finetuning\", False):\n",
        "        print(\"\\nüìå Fine-tuning post-distillation d√©sactiv√©\")\n",
        "        return student_model\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"√âTAPE 10 : FINE-TUNING POST-DISTILLATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # D√©geler toutes les couches\n",
        "    for param in student_model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Nouvel optimizer avec learning rate plus faible\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        student_model.parameters(),\n",
        "        lr=config.get(\"finetuning_lr\", 1e-5),\n",
        "        weight_decay=config[\"weight_decay\"]\n",
        "    )\n",
        "\n",
        "    # Loss standard (sans distillation)\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_acc = 0\n",
        "\n",
        "    for epoch in range(config.get(\"finetuning_epochs\", 3)):\n",
        "        print(f\"\\nFine-tuning Epoch {epoch+1}/{config.get('finetuning_epochs', 3)}\")\n",
        "\n",
        "        # Training\n",
        "        student_model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=\"Fine-tuning\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = student_model(**inputs)\n",
        "            loss = ce_loss(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=-1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate_with_loss(student_model, val_loader, ce_loss)\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        print(f\"Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(student_model.state_dict(),\n",
        "                      os.path.join(config[\"save_dir\"], \"best_finetuned_model.pt\"))\n",
        "            print(f\"‚úÖ Meilleur mod√®le fine-tun√© sauvegard√©! (Val Acc: {best_val_acc:.4f})\")\n",
        "\n",
        "    return student_model\n"
      ],
      "metadata": {
        "id": "o1xVwWN6L9OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# FONCTION PRINCIPALE\n",
        "# ======================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Pipeline principal de distillation\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PIPELINE COMPLET DE KNOWLEDGE DISTILLATION\")\n",
        "    print(\"BERT ‚Üí MiniLM sur IMDB\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"D√©marr√© le: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    try:\n",
        "        # √âtape 1: Chargement des donn√©es\n",
        "        data_splits = load_and_prepare_data(CONFIG)\n",
        "\n",
        "        # √âtape 2: Chargement des mod√®les\n",
        "        teacher_model, student_model, teacher_tokenizer, student_tokenizer = load_models(CONFIG)\n",
        "\n",
        "        # √âtape 3: Cr√©ation des datasets\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"√âTAPE 3 : CR√âATION DES DATASETS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Datasets pour teacher\n",
        "        train_dataset_teacher = IMDBDataset(\n",
        "            *data_splits['train'], teacher_tokenizer, CONFIG[\"max_length\"]\n",
        "        )\n",
        "\n",
        "        # Datasets pour student\n",
        "        train_dataset_student = IMDBDataset(\n",
        "            *data_splits['train'], student_tokenizer, CONFIG[\"max_length\"]\n",
        "        )\n",
        "        val_dataset = IMDBDataset(\n",
        "            *data_splits['validation'], student_tokenizer, CONFIG[\"max_length\"]\n",
        "        )\n",
        "        test_dataset = IMDBDataset(\n",
        "            *data_splits['test'], student_tokenizer, CONFIG[\"max_length\"]\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Datasets cr√©√©s\")\n",
        "\n",
        "        # √âtape 4: DataLoaders\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"√âTAPE 4 : CR√âATION DES DATALOADERS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        train_loader = DualDataLoader(\n",
        "            train_dataset_teacher,\n",
        "            train_dataset_student,\n",
        "            CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            num_workers=CONFIG[\"num_workers\"]\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=False,\n",
        "            num_workers=CONFIG[\"num_workers\"],\n",
        "            pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=False,\n",
        "            num_workers=CONFIG[\"num_workers\"],\n",
        "            pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ DataLoaders cr√©√©s\")\n",
        "        print(f\"   Batches par epoch: {len(train_loader)}\")\n",
        "        print(f\"   Taille de batch: {CONFIG['batch_size']}\")\n",
        "\n",
        "        # √âtape 5: Loss (cr√©√©e dans train_with_dual_loader)\n",
        "\n",
        "        # √âtape 6: Entra√Ænement avec distillation\n",
        "        history, teacher_accuracy = train_with_dual_loader(\n",
        "            teacher_model, student_model,\n",
        "            train_loader, val_loader, test_loader,\n",
        "            CONFIG\n",
        "        )\n",
        "\n",
        "        # √âtape 7: Visualisation\n",
        "        plot_training_results(history)\n",
        "\n",
        "        # √âtape 8: Analyse finale\n",
        "        final_accuracy = analyze_final_results(\n",
        "            student_model, test_loader, history, teacher_accuracy\n",
        "        )\n",
        "\n",
        "        # √âtape 9: Sauvegarde\n",
        "        save_final_model(student_model, student_tokenizer, history, CONFIG)\n",
        "\n",
        "        # √âtape 10: Fine-tuning optionnel\n",
        "        if CONFIG.get(\"enable_post_finetuning\", False):\n",
        "            # Cr√©er un nouveau train_loader simple (pas dual)\n",
        "            train_loader_simple = DataLoader(\n",
        "                train_dataset_student,\n",
        "                batch_size=CONFIG[\"batch_size\"],\n",
        "                shuffle=True,\n",
        "                num_workers=CONFIG[\"num_workers\"]\n",
        "            )\n",
        "            student_model = post_distillation_finetuning(\n",
        "                student_model, train_loader_simple, val_loader, CONFIG\n",
        "            )\n",
        "\n",
        "        # R√©sum√© final\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üéâ PIPELINE TERMIN√â AVEC SUCC√àS!\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Performance finale du student: {final_accuracy:.4f}\")\n",
        "        print(f\"Performance du teacher: {teacher_accuracy:.4f}\")\n",
        "        print(f\"Ratio de r√©tention: {final_accuracy/teacher_accuracy:.1%}\")\n",
        "        print(f\"Compression du mod√®le: {110/22:.1f}x (BERT‚ÜíMiniLM)\")\n",
        "\n",
        "        return student_model, history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Erreur: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "feW2qVjkMA1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# POINT D'ENTR√âE PRINCIPAL\n",
        "# ======================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        try:\n",
        "            free_memory = torch.cuda.mem_get_info(0)[0] / 1e9\n",
        "            total_memory = torch.cuda.mem_get_info(0)[1] / 1e9\n",
        "            print(f\"M√©moire GPU: {free_memory:.2f} GB libres sur {total_memory:.2f} GB\")\n",
        "        except:\n",
        "            print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    student_model, history = main()\n",
        "\n",
        "    if student_model is not None:\n",
        "        print(\"\\n‚úÖ Pipeline compl√©t√© avec succ√®s! Le mod√®le distill√© est pr√™t √† √™tre utilis√©.\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Le pipeline a √©chou√©. V√©rifiez les logs ci-dessus.\")\n"
      ],
      "metadata": {
        "id": "I3XjHigRMD8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# CODE D'EX√âCUTION COMPLET\n",
        "# ======================================================================\n",
        "\n",
        "# D'abord, ajouter la fonction plot_teacher_student_comparison √† votre notebook\n",
        "def plot_teacher_student_comparison(history, teacher_accuracy, config):\n",
        "    \"\"\"\n",
        "    Visualisation comparative de l'√©volution du mod√®le teacher vs student\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import seaborn as sns\n",
        "    from matplotlib.gridspec import GridSpec\n",
        "\n",
        "    # Configuration du style\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "    sns.set_palette(\"husl\")\n",
        "\n",
        "    # Cr√©er une figure avec plusieurs sous-graphiques\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    gs = GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # Couleurs personnalis√©es\n",
        "    color_teacher = '#2E86AB'\n",
        "    color_student = '#A23B72'\n",
        "    color_train = '#F18F01'\n",
        "    color_val = '#C73E1D'\n",
        "\n",
        "    # GRAPHIQUE PRINCIPAL\n",
        "    ax1 = fig.add_subplot(gs[0, :2])\n",
        "    ax1.axhline(y=teacher_accuracy, color=color_teacher, linestyle='--',\n",
        "                linewidth=2.5, label=f'Teacher (BERT): {teacher_accuracy:.3f}', alpha=0.8)\n",
        "\n",
        "    epochs = history['epoch']\n",
        "    ax1.plot(epochs, history['train_acc'], color=color_train, linewidth=2,\n",
        "             label='Student Train', marker='o', markersize=4)\n",
        "    ax1.plot(epochs, history['val_acc'], color=color_val, linewidth=2,\n",
        "             label='Student Val', marker='s', markersize=4)\n",
        "    ax1.plot(epochs, history['test_acc'], color=color_student, linewidth=2.5,\n",
        "             label='Student Test', marker='^', markersize=5)\n",
        "\n",
        "    target_performance = teacher_accuracy * 0.9\n",
        "    ax1.axhspan(target_performance, teacher_accuracy, alpha=0.1, color='green',\n",
        "                label=f'Zone cible (90-100% du Teacher)')\n",
        "\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Accuracy', fontsize=12)\n",
        "    ax1.set_title('√âvolution Comparative: Teacher vs Student Performance', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(loc='lower right', fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim([0.5, 1.0])\n",
        "\n",
        "    best_student_acc = max(history['test_acc'])\n",
        "    best_epoch = history['epoch'][history['test_acc'].index(best_student_acc)]\n",
        "    ax1.annotate(f'Best: {best_student_acc:.3f}',\n",
        "                xy=(best_epoch, best_student_acc),\n",
        "                xytext=(best_epoch+1, best_student_acc-0.03),\n",
        "                arrowprops=dict(arrowstyle='->', color=color_student, alpha=0.7),\n",
        "                fontsize=9, color=color_student)\n",
        "\n",
        "    # Performance Retention Rate\n",
        "    ax2 = fig.add_subplot(gs[0, 2])\n",
        "    retention_rates = [acc/teacher_accuracy * 100 for acc in history['test_acc']]\n",
        "    ax2.plot(epochs, retention_rates, color='purple', linewidth=2, marker='D', markersize=4)\n",
        "    ax2.axhline(y=100, color='green', linestyle='--', alpha=0.5, label='100% (Teacher level)')\n",
        "    ax2.axhline(y=90, color='orange', linestyle='--', alpha=0.5, label='90% (Target)')\n",
        "    ax2.fill_between(epochs, 90, retention_rates, where=(np.array(retention_rates) >= 90),\n",
        "                     color='green', alpha=0.2, label='Above target')\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Performance Retention (%)', fontsize=12)\n",
        "    ax2.set_title('Taux de R√©tention de Performance', fontsize=13, fontweight='bold')\n",
        "    ax2.legend(fontsize=9)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_ylim([70, 105])\n",
        "\n",
        "    # Loss Evolution\n",
        "    ax3 = fig.add_subplot(gs[1, 0])\n",
        "    ax3.plot(epochs, history['train_loss'], color=color_train, linewidth=2,\n",
        "             label='Train Loss', marker='o', markersize=3)\n",
        "    ax3.plot(epochs, history['val_loss'], color=color_val, linewidth=2,\n",
        "             label='Val Loss', marker='s', markersize=3)\n",
        "    ax3.set_xlabel('Epoch', fontsize=12)\n",
        "    ax3.set_ylabel('Loss', fontsize=12)\n",
        "    ax3.set_title('√âvolution des Losses', fontsize=13, fontweight='bold')\n",
        "    ax3.legend(fontsize=10)\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # KL vs CE Loss\n",
        "    ax4 = fig.add_subplot(gs[1, 1])\n",
        "    ax4.plot(epochs, history['kl_loss'], color='cyan', linewidth=2,\n",
        "             label='KL Divergence', marker='o', markersize=3)\n",
        "    ax4.plot(epochs, history['ce_loss'], color='magenta', linewidth=2,\n",
        "             label='Cross Entropy', marker='s', markersize=3)\n",
        "    ax4.set_xlabel('Epoch', fontsize=12)\n",
        "    ax4.set_ylabel('Loss Value', fontsize=12)\n",
        "    ax4.set_title('Composants de la Loss de Distillation', fontsize=13, fontweight='bold')\n",
        "    ax4.legend(fontsize=10)\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    # Convergence Speed\n",
        "    ax5 = fig.add_subplot(gs[1, 2])\n",
        "    improvement_rate = []\n",
        "    for i in range(1, len(epochs)):\n",
        "        rate = (history['test_acc'][i] - history['test_acc'][i-1]) * 100\n",
        "        improvement_rate.append(rate)\n",
        "    ax5.bar(epochs[1:], improvement_rate, color='steelblue', alpha=0.7)\n",
        "    ax5.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    ax5.set_xlabel('Epoch', fontsize=12)\n",
        "    ax5.set_ylabel('Am√©lioration (%)', fontsize=12)\n",
        "    ax5.set_title('Taux d\\'Am√©lioration par Epoch', fontsize=13, fontweight='bold')\n",
        "    ax5.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Overfitting Analysis\n",
        "    ax6 = fig.add_subplot(gs[2, 0])\n",
        "    overfitting_gap = [t - v for t, v in zip(history['train_acc'], history['val_acc'])]\n",
        "    ax6.plot(epochs, overfitting_gap, color='red', linewidth=2, marker='o', markersize=4)\n",
        "    ax6.axhline(y=0.1, color='orange', linestyle='--', label='Seuil critique (10%)')\n",
        "    ax6.axhline(y=0.05, color='yellow', linestyle='--', label='Seuil attention (5%)')\n",
        "    ax6.fill_between(epochs, 0, overfitting_gap, where=(np.array(overfitting_gap) > 0.1),\n",
        "                     color='red', alpha=0.3, label='Zone d\\'overfitting')\n",
        "    ax6.set_xlabel('Epoch', fontsize=12)\n",
        "    ax6.set_ylabel('√âcart Train-Val', fontsize=12)\n",
        "    ax6.set_title('Analyse de l\\'Overfitting', fontsize=13, fontweight='bold')\n",
        "    ax6.legend(fontsize=9)\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning Rate\n",
        "    ax7 = fig.add_subplot(gs[2, 1])\n",
        "    ax7.plot(epochs, history['lr'], color='purple', linewidth=2, marker='o', markersize=3)\n",
        "    ax7.set_xlabel('Epoch', fontsize=12)\n",
        "    ax7.set_ylabel('Learning Rate', fontsize=12)\n",
        "    ax7.set_title('Schedule du Learning Rate', fontsize=13, fontweight='bold')\n",
        "    ax7.set_yscale('log')\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "\n",
        "    # Model Compression\n",
        "    ax8 = fig.add_subplot(gs[2, 2])\n",
        "    compression_data = {\n",
        "        'Teacher (BERT)': 110,\n",
        "        'Student (MiniLM)': 22,\n",
        "        'Reduction': 88\n",
        "    }\n",
        "    colors_bar = ['#2E86AB', '#A23B72', '#4CAF50']\n",
        "    bars = ax8.bar(range(len(compression_data)), list(compression_data.values()),\n",
        "                   color=colors_bar, alpha=0.7)\n",
        "    for bar, value in zip(bars, compression_data.values()):\n",
        "        height = bar.get_height()\n",
        "        ax8.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{value}M', ha='center', va='bottom', fontsize=10)\n",
        "    ax8.set_xticks(range(len(compression_data)))\n",
        "    ax8.set_xticklabels(list(compression_data.keys()), rotation=0)\n",
        "    ax8.set_ylabel('Param√®tres (Millions)', fontsize=12)\n",
        "    ax8.set_title(f'Compression: {110/22:.1f}x | R√©tention: {best_student_acc/teacher_accuracy:.1%}',\n",
        "                 fontsize=13, fontweight='bold')\n",
        "    ax8.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Titre g√©n√©ral\n",
        "    fig.suptitle(f'Knowledge Distillation: BERT ‚Üí MiniLM sur IMDB Dataset',\n",
        "                fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "    # Sauvegarder\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(config[\"save_dir\"], \"teacher_student_comparison.png\")\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # R√©sum√©\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä R√âSUM√â DE LA COMPARAISON TEACHER-STUDENT\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Teacher (BERT) Accuracy: {teacher_accuracy:.4f}\")\n",
        "    print(f\"Best Student (MiniLM) Accuracy: {best_student_acc:.4f}\")\n",
        "    print(f\"Performance Retention: {best_student_acc/teacher_accuracy:.1%}\")\n",
        "    print(f\"Compression Ratio: {110/22:.1f}x\")\n",
        "    print(f\"Epochs to convergence: {best_epoch}\")\n",
        "\n",
        "    final_retention = retention_rates[-1]\n",
        "    if final_retention >= 95:\n",
        "        print(\"‚úÖ Excellente distillation! (>95% retention)\")\n",
        "    elif final_retention >= 90:\n",
        "        print(\"‚úÖ Bonne distillation (90-95% retention)\")\n",
        "    elif final_retention >= 85:\n",
        "        print(\"‚ö†Ô∏è Distillation acceptable (85-90% retention)\")\n",
        "    else:\n",
        "        print(\"‚ùå Distillation sous-optimale (<85% retention)\")\n",
        "\n",
        "    print(f\"\\nGraphique sauvegard√©: {save_path}\")\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# EX√âCUTION DU PIPELINE COMPLET AVEC VISUALISATION\n",
        "# ======================================================================\n",
        "\n",
        "# Option 1: Si vous n'avez PAS encore ex√©cut√© main()\n",
        "if 'student_model' not in globals():\n",
        "    print(\"Lancement du pipeline d'entra√Ænement...\")\n",
        "    student_model, history = main()\n",
        "\n",
        "    # Si l'entra√Ænement s'est bien pass√©\n",
        "    if student_model is not None and history is not None:\n",
        "        # R√©cup√©rer teacher_accuracy depuis l'historique ou recalculer\n",
        "        if 'teacher_accuracy' not in globals():\n",
        "            # Recharger les mod√®les et donn√©es pour l'√©valuation\n",
        "            teacher_model, _, teacher_tokenizer, student_tokenizer = load_models(CONFIG)\n",
        "            data_splits = load_and_prepare_data(CONFIG)\n",
        "            test_dataset = IMDBDataset(\n",
        "                *data_splits['test'], teacher_tokenizer, CONFIG[\"max_length\"]\n",
        "            )\n",
        "            test_loader = DataLoader(\n",
        "                test_dataset,\n",
        "                batch_size=CONFIG[\"batch_size\"],\n",
        "                shuffle=False,\n",
        "                num_workers=CONFIG[\"num_workers\"]\n",
        "            )\n",
        "            teacher_accuracy = evaluate_model(teacher_model, test_loader)\n",
        "\n",
        "        # Cr√©er la visualisation comparative\n",
        "        fig = plot_teacher_student_comparison(history, teacher_accuracy, CONFIG)\n",
        "        print(\"\\n‚úÖ Visualisation cr√©√©e avec succ√®s!\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Erreur lors de l'entra√Ænement\")\n",
        "\n",
        "# Option 2: Si vous AVEZ d√©j√† ex√©cut√© main() et avez history\n",
        "elif 'history' in globals():\n",
        "    print(\"Utilisation des r√©sultats existants...\")\n",
        "\n",
        "    # Si teacher_accuracy n'existe pas, le calculer\n",
        "    if 'teacher_accuracy' not in globals():\n",
        "        print(\"Calcul de l'accuracy du teacher...\")\n",
        "        teacher_model, _, teacher_tokenizer, student_tokenizer = load_models(CONFIG)\n",
        "        data_splits = load_and_prepare_data(CONFIG)\n",
        "        test_dataset = IMDBDataset(\n",
        "            *data_splits['test'], teacher_tokenizer, CONFIG[\"max_length\"]\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=False,\n",
        "            num_workers=CONFIG[\"num_workers\"]\n",
        "        )\n",
        "        teacher_accuracy = evaluate_model(teacher_model, test_loader)\n",
        "\n",
        "    # Cr√©er la visualisation\n",
        "    fig = plot_teacher_student_comparison(history, teacher_accuracy, CONFIG)\n",
        "    print(\"\\n‚úÖ Visualisation cr√©√©e avec succ√®s!\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Aucune donn√©e d'entra√Ænement trouv√©e.\")\n",
        "    print(\"Veuillez d'abord ex√©cuter: student_model, history = main()\")\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# ALTERNATIVE: Charger des r√©sultats sauvegard√©s\n",
        "# ======================================================================\n",
        "\n",
        "def load_and_visualize_saved_results(save_dir):\n",
        "    \"\"\"\n",
        "    Charge et visualise des r√©sultats sauvegard√©s\n",
        "    \"\"\"\n",
        "    import pickle\n",
        "    import json\n",
        "\n",
        "    try:\n",
        "        # Charger l'historique\n",
        "        history_path = os.path.join(save_dir, \"training_history.json\")\n",
        "        if os.path.exists(history_path):\n",
        "            with open(history_path, 'r') as f:\n",
        "                history = json.load(f)\n",
        "        else:\n",
        "            history_path = os.path.join(save_dir, \"training_history.pkl\")\n",
        "            with open(history_path, 'rb') as f:\n",
        "                history = pickle.load(f)\n",
        "\n",
        "        # Charger la configuration\n",
        "        config_path = os.path.join(save_dir, \"config.json\")\n",
        "        with open(config_path, 'r') as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        # R√©cup√©rer teacher_accuracy (devrait √™tre dans l'historique ou config)\n",
        "        teacher_accuracy = history.get('teacher_accuracy', 0.95)  # Valeur par d√©faut\n",
        "\n",
        "        # Cr√©er la visualisation\n",
        "        fig = plot_teacher_student_comparison(history, teacher_accuracy, config)\n",
        "\n",
        "        return fig, history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors du chargement: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Pour utiliser avec des r√©sultats sauvegard√©s:\n",
        "# fig, history = load_and_visualize_saved_results(\"/content/distillation_minilm\")"
      ],
      "metadata": {
        "id": "SNvfH6hKZ-JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pusher mon code sur Github"
      ],
      "metadata": {
        "id": "HxSS4-iE-sy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration Git\n",
        "!git config --global user.name \"davidfawazsalihou\"\n",
        "!git config --global user.email \"davidfawazsalihou@users.noreply.github.com\"\n",
        "\n",
        "# Cr√©er un dossier pour le projet\n",
        "!mkdir -p /content/Compression-Modele-BERT\n",
        "%cd /content/Compression-Modele-BERT\n",
        "\n",
        "# Initialiser le repository\n",
        "!git init\n",
        "!git remote add origin https://github.com/davidfawazsalihou/Compression-Modele-BERT.git\n"
      ],
      "metadata": {
        "id": "v4XpFtcvFPte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T√©l√©charger le notebook actuel\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "# Sauvegarder le notebook\n",
        "notebook_content = {\n",
        "    \"cells\": [], # Contenu du notebook\n",
        "    \"metadata\": {},\n",
        "    \"nbformat\": 4,\n",
        "    \"nbformat_minor\": 0\n",
        "}\n",
        "\n",
        "# Sauvegarder comme fichier\n",
        "with open('BERT_Distillation.ipynb', 'w') as f:\n",
        "    json.dump(notebook_content, f)\n",
        "\n",
        "# Ajouter un README\n",
        "readme_content = \"\"\"# Compression-Modele-BERT\n",
        "\n",
        "Ce repository contient l'impl√©mentation de la knowledge distillation pour compresser BERT vers MiniLM sur le dataset IMDB.\n",
        "\n",
        "## Fonctionnalit√©s\n",
        "- Pipeline complet de distillation\n",
        "- Optimisation avec AMP (Automatic Mixed Precision)\n",
        "- Early stopping et monitoring\n",
        "- Visualisations d√©taill√©es\n",
        "- √âvaluation compl√®te avec matrice de confusion\n",
        "\n",
        "## Utilisation\n",
        "Ouvrez le notebook `BERT_Distillation.ipynb` dans Google Colab et ex√©cutez toutes les cellules.\n",
        "\"\"\"\n",
        "\n",
        "with open('README.md', 'w') as f:\n",
        "    f.write(readme_content)\n"
      ],
      "metadata": {
        "id": "I-j1UfARFZEg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}